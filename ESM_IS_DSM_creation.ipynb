{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Jupyter Notebook to run the IS-DSM prediction and create figure number 6"
      ],
      "metadata": {
        "id": "BYv2fWowjYjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1ZH1-al079-3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3U2HdC7m75NF"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import pathlib\n",
        "import string\n",
        "import torch\n",
        "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from Bio import SeqIO\n",
        "import itertools\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKx5EsEscFm5"
      },
      "source": [
        "# Functions #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WCJe04A833oN"
      },
      "outputs": [],
      "source": [
        "def generate_mutations(seq, alphabet, zero_based_indexing):\n",
        "    \n",
        "    \"The function lacks a control for non-standard symbols UZOB\"\n",
        "    \n",
        "    mutations = []\n",
        "    off = 1 if not zero_based_indexing else 0\n",
        "\n",
        "    for i in range(len(seq)):\n",
        "        alphabet_ = alphabet.copy()\n",
        "        alphabet_.remove(seq[i])\n",
        "        for j in alphabet_:\n",
        "            mutations.append(seq[i] + str(i + off) + j)\n",
        "    \n",
        "    return mutations\n",
        "\n",
        "def generate_mutations_(seq, alphabet, zero_based_indexing):\n",
        "    \n",
        "    \"The function lacks a control for non-standard symbols UZOB\"\n",
        "    \n",
        "    mutations = []\n",
        "    off = 1 if not zero_based_indexing else 0\n",
        "\n",
        "    for i in range(len(seq)):\n",
        "        alphabet_ = alphabet.copy()\n",
        "        for j in alphabet_:\n",
        "            mutations.append(seq[i] + str(i + off) + j)\n",
        "    \n",
        "    return mutations\n",
        "\n",
        "def remove_insertions(sequence: str) -> str:\n",
        "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
        "    # This is an efficient way to delete lowercase characters and insertion characters from a string\n",
        "    deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
        "    deletekeys[\".\"] = None\n",
        "    deletekeys[\"*\"] = None\n",
        "\n",
        "    translation = str.maketrans(deletekeys)\n",
        "    return sequence.translate(translation)\n",
        "\n",
        "\n",
        "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
        "    \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\n",
        "    \n",
        "    The input file must be in a3m format (although we use the SeqIO fasta parser)\n",
        "    for remove_insertions to work properly.\"\"\"\n",
        "\n",
        "    msa = [\n",
        "        (record.description, remove_insertions(str(record.seq)))\n",
        "        for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)\n",
        "    ]\n",
        "    return msa\n",
        "\n",
        "\n",
        "def create_parser():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Label a deep mutational scan with predictions from an ensemble of ESM-1v models.\"  # noqa\n",
        "    )\n",
        "\n",
        "    # fmt: off\n",
        "    parser.add_argument(\n",
        "        \"--model-location\",\n",
        "        type=str,\n",
        "        help=\"PyTorch model file OR name of pretrained model to download (see README for models)\",\n",
        "        nargs=\"+\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--sequence\",\n",
        "        type=str,\n",
        "        help=\"Base sequence to which mutations were applied\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dms-input\",\n",
        "        type=pathlib.Path,\n",
        "        help=\"CSV file containing the deep mutational scan\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mutation-col\",\n",
        "        type=str,\n",
        "        default=\"mutant\",\n",
        "        help=\"column in the deep mutational scan labeling the mutation as 'AiB'\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dms-output\",\n",
        "        type=pathlib.Path,\n",
        "        help=\"Output file containing the deep mutational scan along with predictions\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--offset-idx\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Offset of the mutation positions in `--mutation-col`\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--scoring-strategy\",\n",
        "        type=str,\n",
        "        default=\"wt-marginals\",\n",
        "        choices=[\"wt-marginals\", \"pseudo-ppl\", \"masked-marginals\"],\n",
        "        help=\"\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--msa-path\",\n",
        "        type=pathlib.Path,\n",
        "        help=\"path to MSA in a3m format (required for MSA Transformer)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--msa-samples\",\n",
        "        type=int,\n",
        "        default=400,\n",
        "        help=\"number of sequences to select from the start of the MSA\"\n",
        "    )\n",
        "    # fmt: on\n",
        "    parser.add_argument(\"--nogpu\", action=\"store_true\", help=\"Do not use GPU even if available\")\n",
        "    return parser\n",
        "\n",
        "\n",
        "def label_row(row, sequence, token_probs, alphabet, offset_idx):\n",
        "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
        "    assert sequence[idx] == wt, print(\"The listed wildtype does not match the provided sequence\", idx, sequence[idx], wt, mt)\n",
        "\n",
        "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
        "\n",
        "    # add 1 for BOS\n",
        "    score = token_probs[0, 1 + idx, mt_encoded] - token_probs[0, 1 + idx, wt_encoded]\n",
        "    return score.item()\n",
        "\n",
        "def label_row_likelihood(row, sequence, token_probs, alphabet, offset_idx):\n",
        "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
        "    assert sequence[idx] == wt, print(\"The listed wildtype does not match the provided sequence\", idx, sequence[idx], wt, mt)\n",
        "\n",
        "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
        "\n",
        "    # add 1 for BOS\n",
        "    score = token_probs[0, 1 + idx, mt_encoded]\n",
        "    return score.item()\n",
        "\n",
        "\n",
        "def compute_pppl(row, sequence, model, alphabet, offset_idx):\n",
        "    wt, idx, mt = row[0], int(row[1:-1]) - offset_idx, row[-1]\n",
        "    print(idx)\n",
        "    assert sequence[idx] == wt, \"The listed wildtype does not match the provided sequence\"\n",
        "\n",
        "    # modify the sequence\n",
        "    sequence = sequence[:idx] + mt + sequence[(idx + 1) :]\n",
        "\n",
        "    # encode the sequence\n",
        "    data = [\n",
        "        (\"protein1\", sequence),\n",
        "    ]\n",
        "\n",
        "    batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
        "\n",
        "    # compute probabilities at each position\n",
        "    log_probs = []\n",
        "    for i in range(1, len(sequence) - 1):\n",
        "        batch_tokens_masked = batch_tokens.clone()\n",
        "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
        "        with torch.no_grad():\n",
        "            token_probs = torch.log_softmax(model(batch_tokens_masked.cuda())[\"logits\"], dim=-1)\n",
        "        log_probs.append(token_probs[0, i, alphabet.get_idx(sequence[i])].item())  # vocab size\n",
        "    return sum(log_probs)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Load the deep mutational scan\n",
        "    df = pd.read_csv(args[\"dms_input\"])\n",
        "\n",
        "    # inference for each model\n",
        "    for model_location in args[\"model_location\"]:\n",
        "        model, alphabet = pretrained.load_model_and_alphabet(model_location)\n",
        "        model.eval()\n",
        "        \n",
        "        model = model.cuda()\n",
        "        print(\"Transferred model to GPU\")\n",
        "\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "        \n",
        "        \n",
        "        data = [(\"protein1\", args[\"sequence\"]), ]\n",
        "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "        if args[\"scoring_strategy\"] == \"wt-marginals\":\n",
        "            with torch.no_grad():\n",
        "                token_probs = torch.log_softmax(model(batch_tokens.cuda())[\"logits\"], dim=-1)\n",
        "            df[model_location] = df.apply(\n",
        "                lambda row: label_row(\n",
        "                    row[args[\"mutation_col\"]],\n",
        "                    args[\"sequence\"],\n",
        "                    token_probs,\n",
        "                    alphabet,\n",
        "                    args[\"offset_idx\"],\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "        elif args[\"scoring_strategy\"] == \"masked-marginals\":\n",
        "            all_token_probs = []\n",
        "            for i in tqdm(range(batch_tokens.size(1))):\n",
        "                batch_tokens_masked = batch_tokens.clone()\n",
        "                batch_tokens_masked[0, i] = alphabet.mask_idx\n",
        "                with torch.no_grad():\n",
        "                    token_probs = torch.log_softmax(\n",
        "                        model(batch_tokens_masked.cuda())[\"logits\"], dim=-1\n",
        "                    )\n",
        "                all_token_probs.append(token_probs[:, i])  # vocab size\n",
        "            token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
        "            df[model_location] = df.apply(\n",
        "                lambda row: label_row(\n",
        "                    row[args[\"mutation_col\"]],\n",
        "                    args[\"sequence\"],\n",
        "                    token_probs,\n",
        "                    alphabet,\n",
        "                    args[\"offset_idx\"],\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "        elif args[\"scoring_strategy\"] == \"pseudo-ppl\":\n",
        "            tqdm.pandas()\n",
        "            df[model_location] = df.progress_apply(\n",
        "                lambda row: compute_pppl(\n",
        "                    row[args[\"mutation_col\"]], args[\"sequence\"], model, alphabet, args[\"offset_idx\"]\n",
        "                ),\n",
        "                axis=1,\n",
        "            )\n",
        "\n",
        "    df.to_csv(args[\"dms_output\"])\n",
        "\n",
        "def main_only_likelihood(args):\n",
        "    # Load the deep mutational scan\n",
        "    df = pd.read_csv(args[\"dms_input\"])\n",
        "\n",
        "    # inference for each model\n",
        "    for model_location in args[\"model_location\"]:\n",
        "        model, alphabet = pretrained.load_model_and_alphabet(model_location)\n",
        "        model.eval()\n",
        "        model = model.cuda()\n",
        "        print(\"Transferred model to GPU\")\n",
        "\n",
        "        batch_converter = alphabet.get_batch_converter()\n",
        "        data = [(\"protein1\", args[\"sequence\"]), ]\n",
        "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "\n",
        "        all_token_probs = []\n",
        "        for i in tqdm(range(batch_tokens.size(1))):\n",
        "            batch_tokens_masked = batch_tokens.clone()\n",
        "            batch_tokens_masked[0, i] = alphabet.mask_idx\n",
        "            with torch.no_grad():\n",
        "                token_probs = torch.log_softmax( model(batch_tokens_masked.cuda())[\"logits\"], dim=-1)\n",
        "            all_token_probs.append(token_probs[:, i])  # vocab size\n",
        "        token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
        "        df[model_location] = df.apply( lambda row: label_row_likelihood( row[args[\"mutation_col\"]], args[\"sequence\"], token_probs, alphabet, args[\"offset_idx\"] ), axis=1 )\n",
        "\n",
        "    df.to_csv(args[\"dms_output\"])\n",
        "\n",
        "def alphabet_to_mat_position(x, alphabet):\n",
        "    return alphabet[x]\n",
        "\n",
        "def mat_position_to_alphabet(x, alphabet):\n",
        "    inv_map = {v: k for k, v in alphabet.items()}\n",
        "    return inv_map[x]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4d8LT579pPk"
      },
      "source": [
        "# IS-DMS creation #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaoVnzvu968t"
      },
      "outputs": [],
      "source": [
        "#Create a dictionary with all possible residues\n",
        "alph = {'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5 ,'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19}\n",
        "Sequence_to_be_ran = \"Paste your sequence here\"\n",
        "Length_of_protein = len(Sequence_to_be_ran)\n",
        "#Generates all possible mutations in every position\n",
        "mutations_to_hyperactive = generate_mutations(Sequence_to_be_ran, alphabet = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'K', 'V', 'Y', 'W'], zero_based_indexing = False)\n",
        "\n",
        "#Generates a pandas dataframe with all the mutation\n",
        "sequence_data_frame = pd.DataFrame({'mutant': mutations_to_hyperactive, 'bla': np.nan})\n",
        "\n",
        "#Transforms the data frame to a CSV file\n",
        "sequence_data_frame.to_csv('Hyperactive_DMS.csv')\n",
        "\n",
        "#Creates a library with all the necessary values to run the main function, the different models (esm1v_t33_650M_UR90S_#.pt) must have been previously downloaded and their location must be given in model_location\n",
        "args_ens_1 = {\"model_location\": [\"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_1.pt\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_2.pt\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_3.pt\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_4.pt\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_5.pt\"], \n",
        "        \"sequence\": Sequence_to_be_ran, \n",
        "        \"dms_input\": \"Hyperactive_DMS.csv\",\n",
        "        \"mutation_col\": \"mutant\", \n",
        "        \"dms_output\": \"Hyperactive_DMS.csv\", \n",
        "        \"offset_idx\": 1, \n",
        "        \"scoring_strategy\": \"masked-marginals\"}\n",
        "\n",
        "#Runs ESM-1v variant prediction\n",
        "main(args_ens_1)\n",
        "\n",
        "#Main 1 writes the calculated fitness score for each possible mutation for each model directly to Hyperactive_DMS.csv "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create figure 6 #"
      ],
      "metadata": {
        "id": "kCnaNdNK55vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop extra columns from Hyperactive_DMS.csv\n",
        "Seq_df = pd.read_csv('Hyperactive_DMS.csv').drop(['Unnamed: 0.1', 'Unnamed: 0', 'bla'], axis = 1)\n",
        "\n",
        "#Rename the columns with the model names\n",
        "Seq_df = Seq_df.rename(columns={\"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_1.pt\": \"esm1v_t33_650M_UR90S_1\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_2.pt\": \"esm1v_t33_650M_UR90S_2\",\"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_3.pt\": \"esm1v_t33_650M_UR90S_3\",\"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_4.pt\": \"esm1v_t33_650M_UR90S_4\", \"/content/drive/Shareddrives/ESM_for_variant_prediction/esm1v_t33_650M_UR90S_5.pt\": \"esm1v_t33_650M_UR90S_5\"})\n",
        "\n",
        "#Create a column with the mean score for each mutation from all the models\n",
        "Seq_df['ensemble'] = Seq_df[['esm1v_t33_650M_UR90S_1', 'esm1v_t33_650M_UR90S_2', 'esm1v_t33_650M_UR90S_3', 'esm1v_t33_650M_UR90S_4', 'esm1v_t33_650M_UR90S_5']].mean(axis=1)\n",
        "\n",
        "#Create a column to number the mutations according to their position\n",
        "Seq_df['values'] = [int(i[1:-1]) - 1 for i in Seq_df['mutant'].values]\n",
        "\n",
        "#Create a column to number the mutation according to what residue is the mutation\n",
        "Seq_df['mutation_mat_position'] = [alphabet_to_mat_position(i[-1], alph) for i in Seq_df['mutant'].values]\n",
        "\n",
        "#Create two numpy arrays where the positions of the residues are grouped together with the mean value in temp_1 and with the type of residue in temp_2\n",
        "temp_1 = np.array(list(Seq_df[['values', 'ensemble']].groupby('values').ensemble.apply(list).reset_index()['ensemble'].values))\n",
        "temp_2 = np.array(list(Seq_df[['values', 'mutation_mat_position']].groupby('values').mutation_mat_position.apply(list).reset_index()['mutation_mat_position'].values))\n",
        "\n",
        "\n",
        "#Create a list of lists of zero values according to the length of the sequence and all possible mutations(20)\n",
        "full_mutations_matrix_HPB = np.zeros((Length_of_protein, 20))*np.nan\n",
        "\n",
        "#Loop through length of the protein to fill the full_mutations_matrix with the mutation values\n",
        "for j in range(Length_of_protein):\n",
        "    for i in zip(temp_1[j, :], temp_2[j, :]):\n",
        "      full_mutations_matrix_HPB[j, i[1]] = i[0]\n",
        "\n",
        "#Delete temp_1 and temp_2\n",
        "del temp_1, temp_2\n",
        "\n",
        "#Create the figure and axis to plot the mutation scores\n",
        "fig, ax = plt.subplots(1, figsize = (12, 5))\n",
        "\n",
        "#Create an axvspan according to the different domains of the piggyBac\n",
        "ax.axvspan(xmin=0,  xmax=116,   ymin=0, linewidth=0, color='grey',     alpha = 0.1, label = 'N-terminal');\n",
        "ax.axvspan(xmin=116,  xmax=262,   ymin=0, linewidth=0, color='yellow',     alpha = 0.1, label = 'DDBD');\n",
        "ax.axvspan(xmin=262,  xmax=371,   ymin=0, linewidth=0, color='green',     alpha = 0.1, label = 'Catalitic_domain');\n",
        "ax.axvspan(xmin=371,  xmax=432,   ymin=0, linewidth=0, color='blue',     alpha = 0.1, label = 'Insertion_domain');\n",
        "ax.axvspan(xmin=432,  xmax=456,   ymin=0, linewidth=0, color='green',     alpha = 0.1, label = 'Catalitic_domain');\n",
        "ax.axvspan(xmin=456,  xmax=534,   ymin=0, linewidth=0, color='yellow',     alpha = 0.1, label = 'DDBD');\n",
        "ax.axvspan(xmin=534,  xmax=552,   ymin=0, linewidth=0, color='grey',     alpha = 0.1, label = '--');\n",
        "ax.axvspan(xmin=552,  xmax=593,   ymin=0, linewidth=0, color='red',     alpha = 0.1, label = 'CRD');\n",
        "\n",
        "\n",
        "#Set ticks to mark the position of the different domains in the x axis\n",
        "ax.set_xticks([0, 116, 262, 371, 432, 456, 534, 552, 593])\n",
        "ax.set_xticklabels([1, 117 , 263, 372, 433 , 457, 535 , 553, 594 ])\n",
        "\n",
        "\n",
        "\n",
        "#Loop through all the possible mutations at each position and plot them in a scater plot\n",
        "for i in range(20):\n",
        "    ax.scatter(np.arange(0, full_mutations_matrix_HPB.shape[0]), full_mutations_matrix_HPB[:, i], s = 0.5, color = 'blue', alpha = 0.5)\n",
        "\n",
        "# Set the legends and spine    \n",
        "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
        "legend = ax.legend();\n",
        "for lh in legend.legendHandles: \n",
        "    lh.set_alpha(1)\n",
        "\n",
        "#Create a red doted line to mark the 0 value on the graph \n",
        "ax.axhline(y=0, color='r', linestyle='--', alpha = 0.5);\n"
      ],
      "metadata": {
        "id": "_eKMBR0A53WH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot heatmap with fitness scores"
      ],
      "metadata": {
        "id": "le6z_xxDzjU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = ''.join(alph.keys())\n",
        "\n",
        "#Select the residues to be ploted together with all its possible mutations and scores in a heatmap\n",
        "residues = [1,2,5,6]\n",
        "matrix = full_mutations_matrix_HPB[residues, :]\n",
        "\n",
        "# Create a heatmap of the mutations\n",
        "fig, ax = plt.subplots(figsize = (30, 100))\n",
        "im = ax.imshow(matrix, cmap='RdYlBu')\n",
        "\n",
        "ax.set_xticks(np.arange(len(alphabet)))\n",
        "ax.set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
        "ax.set_xticklabels(list(alphabet))\n",
        "ax.set_yticklabels(residues)\n",
        "\n",
        "# Rotate the x-axis labels for better readability\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\");\n",
        "\n",
        "cbar = ax.figure.colorbar(im, ax=ax, fraction = 0.01)\n",
        "\n",
        "for i in range(len(residues)):\n",
        "    for j in range(len(alphabet)):\n",
        "        text = ax.text(j, i, round(matrix[i, j], 2), ha=\"center\", va=\"center\", color=\"black\", size = 20)"
      ],
      "metadata": {
        "id": "PzVzXwBkwZSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MKx5EsEscFm5",
        "2VlEChttWgRW",
        "5VB8utiiWk-X",
        "nsoU4Doi2_I6",
        "8COjcBA7C5a7",
        "QGqQnnjTr_Qa",
        "v5cpD4-y_joD",
        "qhj7R2B__mQx",
        "4dQDVlF5AUV0",
        "xhXN6krnPRAX",
        "SX3HN5LyHWgk",
        "ha66P8rhsOwz",
        "jIsCKnwtanth",
        "OQ2soN0N2S9e",
        "dOJKHohlT2yU",
        "Rdkxut2FPsTQ",
        "ZVeJf4CG_we9"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}